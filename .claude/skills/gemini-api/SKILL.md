---
name: gemini-api
description: Google Gemini API for AI/ML applications. Use for text generation, image understanding, video processing, function calling, embeddings, and multimodal AI capabilities.
---

# Gemini-Api Skill

Comprehensive assistance with gemini-api development, generated from official documentation.

## When to Use This Skill

This skill should be triggered when:
- Working with gemini-api
- Asking about gemini-api features or APIs
- Implementing gemini-api solutions
- Debugging gemini-api code
- Learning gemini-api best practices

## Quick Reference

### Common Patterns

**Pattern 1:** Use this guide to help you diagnose and resolve common issues that arise when you call the Gemini API. You may encounter issues from either the Gemini API backend service or the client SDKs. Our client SDKs are open sourced in the following repositories: python-genai js-genai go-genai If you encounter API key issues, verify that you have set up your API key correctly per the API key setup guide. Gemini API backend service error codes The following table lists common backend error codes you may encounter, along with explanations for their causes and troubleshooting steps: HTTP Code Status Description Example Solution 400 INVALID_ARGUMENT The request body is malformed. There is a typo, or a missing required field in your request. Check the API reference for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors. 400 FAILED_PRECONDITION Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio. You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio. To use the Gemini API, you will need to setup a paid plan using Google AI Studio. 403 PERMISSION_DENIED Your API key doesn't have the required permissions. You are using the wrong API key; you are trying to use a tuned model without going through proper authentication. Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models. 404 NOT_FOUND The requested resource wasn't found. An image, audio, or video file referenced in your request was not found. Check if all parameters in your request are valid for your API version. 429 RESOURCE_EXHAUSTED You've exceeded the rate limit. You are sending too many requests per minute with the free tier Gemini API. Verify that you're within the model's rate limit. Request a quota increase if needed. 500 INTERNAL An unexpected error occurred on Google's side. Your input context is too long. Reduce your input context or temporarily switch to another model (e.g. from Gemini 2.5 Pro to Gemini 2.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 503 UNAVAILABLE The service may be temporarily overloaded or down. The service is temporarily running out of capacity. Temporarily switch to another model (e.g. from Gemini 2.5 Pro to Gemini 2.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the Send feedback button in Google AI Studio. 504 DEADLINE_EXCEEDED The service is unable to finish processing within the deadline. Your prompt (or context) is too large to be processed in time. Set a larger 'timeout' in your client request to avoid this error. Check your API calls for model parameter errors Verify that your model parameters are within the following values: Model parameter Values (range) Candidate count 1-8 (integer) Temperature 0.0-1.0 Max output tokens Use get_model (Python) to determine the maximum number of tokens for the model you are using. TopP 0.0-1.0 In addition to checking parameter values, make sure you're using the correct API version (e.g., /v1 or /v1beta) and model that supports the features you need. For example, if a feature is in Beta release, it will only be available in the /v1beta API version. Check if you have the right model Verify that you are using a supported model listed on our models page. Higher latency or token usage with 2.5 models If you're observing higher latency or token usage with the 2.5 Flash and Pro models, this can be because they come with thinking is enabled by default in order to enhance quality. If you are prioritizing speed or need to minimize costs, you can adjust or disable thinking. Refer to thinking page for guidance and sample code. Safety issues If you see a prompt was blocked because of a safety setting in your API call, review the prompt with respect to the filters you set in the API call. If you see BlockedReason.OTHER, the query or response may violate the terms of service or be otherwise unsupported. Recitation issue If you see the model stops generating output due to the RECITATION reason, this means the model output may resemble certain data. To fix this, try to make prompt / context as unique as possible and use a higher temperature. When using Gemini 3 models, we strongly recommend keeping the temperature at its default value of 1.0. Changing the temperature (setting it below 1.0) may lead to unexpected behavior, such as looping or degraded performance, particularly in complex mathematical or reasoning tasks. Repetitive tokens issue If you see repeated output tokens, try the following suggestions to help reduce or eliminate them. Description Cause Suggested workaround Repeated hyphens in Markdown tables This can occur when the contents of the table are long as the model tries to create a visually aligned Markdown table. However, the alignment in Markdown is not necessary for correct rendering. Add instructions in your prompt to give the model specific guidelines for generating Markdown tables. Provide examples that follow those guidelines. You can also try adjusting the temperature. For generating code or very structured output like Markdown tables, high temperature have shown to work better (>= 0.8). The following is an example set of guidelines you can add to your prompt to prevent this issue: # Markdown Table Format * Separator line: Markdown tables must include a separator line below the header row. The separator line must use only 3 hyphens per column, for example: |---|---|---|. Using more hypens like ----, -----, ------ can result in errors. Always use |:---|, |---:|, or |---| in these separator strings. For example: | Date | Description | Attendees | |---|---|---| | 2024-10-26 | Annual Conference | 500 | | 2025-01-15 | Q1 Planning Session | 25 | * Alignment: Do not align columns. Always use |---|. For three columns, use |---|---|---| as the separator line. For four columns use |---|---|---|---| and so on. * Conciseness: Keep cell content brief and to the point. * Never pad column headers or other cells with lots of spaces to match with width of other content. Only a single space on each side is needed. For example, always do "| column name |" instead of "| column name |". Extra spaces are wasteful. A markdown renderer will automatically take care displaying the content in a visually appealing form. Repeated tokens in Markdown tables Similar to the repeated hyphens, this occurs when the model tries to visually align the contents of the table. The alignment in Markdown is not required for correct rendering. Try adding instructions like the following to your system prompt: FOR TABLE HEADINGS, IMMEDIATELY ADD ' |' AFTER THE TABLE HEADING. Try adjusting the temperature. Higher temperatures (>= 0.8) generally helps to eliminate repetitions or duplication in the output. Repeated newlines (\n) in structured output When the model input contains unicode or escape sequences like \u or \t, it can lead to repeated newlines. Check for and replace forbidden escape sequences with UTF-8 characters in your prompt. For example, \u escape sequence in your JSON examples can cause the model to use them in its output too. Instruct the model on allowed escapes. Add a system instruction like this: In quoted strings, the only allowed escape sequences are \\, \n, and \". Instead of \u escapes, use UTF-8. Repeated text in using structured output When the model output has a different order for the fields than the defined structured schema, this can lead to repeating text. Don't specify the order of fields in your prompt. Make all output fields required. Repetitive tool calling This can occur if the model loses the context of previous thoughts and/or call an unavailable endpoint that it's forced to. Instruct the model to maintain state within its thought process. Add this to the end of your system instructions: When thinking silently: ALWAYS start the thought with a brief (one sentence) recap of the current progress on the task. In particular, consider whether the task is already done. Repetitive text that's not part of structured output This can occur if the model gets stuck on a request that it can't resolve. If thinking is turned on, avoid giving explicit orders for how to think through a problem in the instructions. Just ask for the final output. Try a higher temperature >= 0.8. Add instructions like "Be concise", "Don't repeat yourself", or "Provide the answer once". Blocked or non-working API keys This section describes how to check whether your Gemini API key is blocked and what to do about it. Understand why keys are blocked We have identified a vulnerability where some API keys may have been publicly exposed. To protect your data and prevent unauthorized access, we have proactively blocked these known leaked keys from accessing the Gemini API. Confirm if your keys are affected If your key is known to be leaked, you can no longer use that key with the Gemini API. You can use Google AI Studio to see if any of your API keys are blocked from calling the Gemini API and generate new keys. You may also see the following error returned when attempting to use these keys: Your API key was reported as leaked. Please use another API key. Action for blocked API keys You should generate new API keys for your Gemini API integrations using Google AI Studio. We strongly recommend reviewing your API key management practices to ensure that your new keys are kept secure and are not publicly exposed. Unexpected charges due to vulnerability Submit a billing support case. Our billing team is working on this, and we will communicate updates as soon as possible. Google's security measures for leaked keys How is Google going to help secure my account from cost overrun and abuse if my API keys are leaked? We are moving towards issuing API keys when you request a new key using Google AI Studio that will by default be limited to only Google AI Studio and not accept keys from other services. This will help prevent any unintended cross-key usage. We are defaulting to blocking API keys that are leaked and used with the Gemini API, helping prevent abuse of cost and your application data. You will be able to find the status of your API keys within Google AI Studio and we will work on communicating proactively when we identify your API keys are leaked for immediate action. Improve model output For higher quality model outputs, explore writing more structured prompts. The prompt engineering guide page introduces some basic concepts, strategies, and best practices to get you started. Understand token limits Read through our Token guide to better understand how to count tokens and their limits. Known issues The API supports only a number of select languages. Submitting prompts in unsupported languages can produce unexpected or even blocked responses. See available languages for updates. File a bug Join the discussion on the Google AI developer forum if you have questions.

```
get_model
```

**Pattern 2:** The Gemini 3 and 2.5 series models use an internal "thinking process" that significantly improves their reasoning and multi-step planning abilities, making them highly effective for complex tasks such as coding, advanced mathematics, and data analysis. This guide shows you how to work with Gemini's thinking capabilities using the Gemini API. Generating content with thinking Initiating a request with a thinking model is similar to any other content generation request. The key difference lies in specifying one of the models with thinking support in the model field, as demonstrated in the following text generation example: Pythonfrom google import genai client = genai.Client() prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example." response = client.models.generate_content( model="gemini-2.5-pro", contents=prompt ) print(response.text) JavaScriptimport { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); async function main() { const prompt = "Explain the concept of Occam's Razor and provide a simple, everyday example."; const response = await ai.models.generateContent({ model: "gemini-2.5-pro", contents: prompt, }); console.log(response.text); } main(); Gopackage main import ( "context" "fmt" "log" "os" "google.golang.org/genai" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } prompt := "Explain the concept of Occam's Razor and provide a simple, everyday example." model := "gemini-2.5-pro" resp, _ := client.Models.GenerateContent(ctx, model, genai.Text(prompt), nil) fmt.Println(resp.Text()) } RESTcurl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H 'Content-Type: application/json' \ -X POST \ -d '{ "contents": [ { "parts": [ { "text": "Explain the concept of Occam\'s Razor and provide a simple, everyday example." } ] } ] }' ``` Thought summaries Thought summaries are synthesized versions of the model's raw thoughts and offer insights into the model's internal reasoning process. Note that thinking levels and budgets apply to the model's raw thoughts and not to thought summaries. You can enable thought summaries by setting includeThoughts to true in your request configuration. You can then access the summary by iterating through the response parameter's parts, and checking the thought boolean. Here's an example demonstrating how to enable and retrieve thought summaries without streaming, which returns a single, final thought summary with the response: Pythonfrom google import genai from google.genai import types client = genai.Client() prompt = "What is the sum of the first 50 prime numbers?" response = client.models.generate_content( model="gemini-2.5-pro", contents=prompt, config=types.GenerateContentConfig( thinking_config=types.ThinkingConfig( include_thoughts=True ) ) ) for part in response.candidates[0].content.parts: if not part.text: continue if part.thought: print("Thought summary:") print(part.text) print() else: print("Answer:") print(part.text) print() JavaScriptimport { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); async function main() { const response = await ai.models.generateContent({ model: "gemini-2.5-pro", contents: "What is the sum of the first 50 prime numbers?", config: { thinkingConfig: { includeThoughts: true, }, }, }); for (const part of response.candidates[0].content.parts) { if (!part.text) { continue; } else if (part.thought) { console.log("Thoughts summary:"); console.log(part.text); } else { console.log("Answer:"); console.log(part.text); } } } main(); Gopackage main import ( "context" "fmt" "google.golang.org/genai" "os" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } contents := genai.Text("What is the sum of the first 50 prime numbers?") model := "gemini-2.5-pro" resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{ ThinkingConfig: &genai.ThinkingConfig{ IncludeThoughts: true, }, }) for _, part := range resp.Candidates[0].Content.Parts { if part.Text != "" { if part.Thought { fmt.Println("Thoughts Summary:") fmt.Println(part.Text) } else { fmt.Println("Answer:") fmt.Println(part.Text) } } } } And here is an example using thinking with streaming, which returns rolling, incremental summaries during generation: Pythonfrom google import genai from google.genai import types client = genai.Client() prompt = """ Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue. The person who lives in the red house owns a cat. Bob does not live in the green house. Carol owns a dog. The green house is to the left of the red house. Alice does not own a cat. Who lives in each house, and what pet do they own? """ thoughts = "" answer = "" for chunk in client.models.generate_content_stream( model="gemini-2.5-pro", contents=prompt, config=types.GenerateContentConfig( thinking_config=types.ThinkingConfig( include_thoughts=True ) ) ): for part in chunk.candidates[0].content.parts: if not part.text: continue elif part.thought: if not thoughts: print("Thoughts summary:") print(part.text) thoughts += part.text else: if not answer: print("Answer:") print(part.text) answer += part.text JavaScriptimport { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); const prompt = `Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue. The person who lives in the red house owns a cat. Bob does not live in the green house. Carol owns a dog. The green house is to the left of the red house. Alice does not own a cat. Who lives in each house, and what pet do they own?`; let thoughts = ""; let answer = ""; async function main() { const response = await ai.models.generateContentStream({ model: "gemini-2.5-pro", contents: prompt, config: { thinkingConfig: { includeThoughts: true, }, }, }); for await (const chunk of response) { for (const part of chunk.candidates[0].content.parts) { if (!part.text) { continue; } else if (part.thought) { if (!thoughts) { console.log("Thoughts summary:"); } console.log(part.text); thoughts = thoughts + part.text; } else { if (!answer) { console.log("Answer:"); } console.log(part.text); answer = answer + part.text; } } } } await main(); Gopackage main import ( "context" "fmt" "log" "os" "google.golang.org/genai" ) const prompt = ` Alice, Bob, and Carol each live in a different house on the same street: red, green, and blue. The person who lives in the red house owns a cat. Bob does not live in the green house. Carol owns a dog. The green house is to the left of the red house. Alice does not own a cat. Who lives in each house, and what pet do they own? ` func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } contents := genai.Text(prompt) model := "gemini-2.5-pro" resp := client.Models.GenerateContentStream(ctx, model, contents, &genai.GenerateContentConfig{ ThinkingConfig: &genai.ThinkingConfig{ IncludeThoughts: true, }, }) for chunk := range resp { for _, part := range chunk.Candidates[0].Content.Parts { if len(part.Text) == 0 { continue } if part.Thought { fmt.Printf("Thought: %s\n", part.Text) } else { fmt.Printf("Answer: %s\n", part.Text) } } } } Controlling thinking Gemini models engage in dynamic thinking by default, automatically adjusting the amount of reasoning effort based on the complexity of the user's request. However, if you have specific latency constraints or require the model to engage in deeper reasoning than usual, you can optionally use parameters to control thinking behavior. Thinking levels (Gemini 3) The thinkingLevel parameter, recommended for Gemini 3 models and onwards, lets you control reasoning behavior. You can set thinking level to "low" or "high" for Gemini 3 Pro, and "minimal", "low", "medium", and "high" for Gemini 3 Flash. Gemini 3 Pro and Flash thinking levels: low: Minimizes latency and cost. Best for simple instruction following, chat, or high-throughput applications high (Default, dynamic): Maximizes reasoning depth. The model may take significantly longer to reach a first token, but the output will be more carefully reasoned. Gemini 3 Flash thinking levels In addition to the levels above, Gemini 3 Flash also supports the following thinking levels that are not currently supported by Gemini 3 Pro: medium: Balanced thinking for most tasks. minimal: Matches the "no thinking" setting for most queries. The model may think very minimally for complex coding tasks. Minimizes latency for chat or high throughput applications. Note: minimal does not guarantee that thinking is off. Pythonfrom google import genai from google.genai import types client = genai.Client() response = client.models.generate_content( model="gemini-3-flash-preview", contents="Provide a list of 3 famous physicists and their key contributions", config=types.GenerateContentConfig( thinking_config=types.ThinkingConfig(thinking_level="low") ), ) print(response.text) JavaScriptimport { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); async function main() { const response = await ai.models.generateContent({ model: "gemini-3-flash-preview", contents: "Provide a list of 3 famous physicists and their key contributions", config: { thinkingConfig: { thinkingLevel: "low", }, }, }); console.log(response.text); } main(); Gopackage main import ( "context" "fmt" "google.golang.org/genai" "os" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } thinkingLevelVal := "low" contents := genai.Text("Provide a list of 3 famous physicists and their key contributions") model := "gemini-3-flash-preview" resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{ ThinkingConfig: &genai.ThinkingConfig{ ThinkingLevel: &thinkingLevelVal, }, }) fmt.Println(resp.Text()) } RESTcurl "https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H 'Content-Type: application/json' \ -X POST \ -d '{ "contents": [ { "parts": [ { "text": "Provide a list of 3 famous physicists and their key contributions" } ] } ], "generationConfig": { "thinkingConfig": { "thinkingLevel": "low" } } }' You cannot disable thinking for Gemini 3 Pro. Gemini 3 Flash also does not support full thinking-off, but the minimal setting means the model likely will not think (though it still potentially can). If you don't specify a thinking level, Gemini will use the Gemini 3 models' default dynamic thinking level, "high". Gemini 2.5 series models don't support thinkingLevel; use thinkingBudget instead. Thinking budgets The thinkingBudget parameter, introduced with the Gemini 2.5 series, guides the model on the specific number of thinking tokens to use for reasoning. Note: Use the thinkingLevel parameter with Gemini 3 models. While thinkingBudget is accepted for backwards compatibility, using it with Gemini 3 Pro may result in suboptimal performance. The following are thinkingBudget configuration details for each model type. You can disable thinking by setting thinkingBudget to 0. Setting the thinkingBudget to -1 turns on dynamic thinking, meaning the model will adjust the budget based on the complexity of the request. Model Default setting(Thinking budget is not set) Range Disable thinking Turn on dynamic thinking 2.5 Pro Dynamic thinking: Model decides when and how much to think 128 to 32768 N/A: Cannot disable thinking thinkingBudget = -1 2.5 Flash Dynamic thinking: Model decides when and how much to think 0 to 24576 thinkingBudget = 0 thinkingBudget = -1 2.5 Flash Preview Dynamic thinking: Model decides when and how much to think 0 to 24576 thinkingBudget = 0 thinkingBudget = -1 2.5 Flash Lite Model does not think 512 to 24576 thinkingBudget = 0 thinkingBudget = -1 2.5 Flash Lite Preview Model does not think 512 to 24576 thinkingBudget = 0 thinkingBudget = -1 Robotics-ER 1.5 Preview Dynamic thinking: Model decides when and how much to think 0 to 24576 thinkingBudget = 0 thinkingBudget = -1 2.5 Flash Live Native Audio Preview (09-2025) Dynamic thinking: Model decides when and how much to think 0 to 24576 thinkingBudget = 0 thinkingBudget = -1 Pythonfrom google import genai from google.genai import types client = genai.Client() response = client.models.generate_content( model="gemini-2.5-flash", contents="Provide a list of 3 famous physicists and their key contributions", config=types.GenerateContentConfig( thinking_config=types.ThinkingConfig(thinking_budget=1024) # Turn off thinking: # thinking_config=types.ThinkingConfig(thinking_budget=0) # Turn on dynamic thinking: # thinking_config=types.ThinkingConfig(thinking_budget=-1) ), ) print(response.text) JavaScriptimport { GoogleGenAI } from "@google/genai"; const ai = new GoogleGenAI({}); async function main() { const response = await ai.models.generateContent({ model: "gemini-2.5-flash", contents: "Provide a list of 3 famous physicists and their key contributions", config: { thinkingConfig: { thinkingBudget: 1024, // Turn off thinking: // thinkingBudget: 0 // Turn on dynamic thinking: // thinkingBudget: -1 }, }, }); console.log(response.text); } main(); Gopackage main import ( "context" "fmt" "google.golang.org/genai" "os" ) func main() { ctx := context.Background() client, err := genai.NewClient(ctx, nil) if err != nil { log.Fatal(err) } thinkingBudgetVal := int32(1024) contents := genai.Text("Provide a list of 3 famous physicists and their key contributions") model := "gemini-2.5-flash" resp, _ := client.Models.GenerateContent(ctx, model, contents, &genai.GenerateContentConfig{ ThinkingConfig: &genai.ThinkingConfig{ ThinkingBudget: &thinkingBudgetVal, // Turn off thinking: // ThinkingBudget: int32(0), // Turn on dynamic thinking: // ThinkingBudget: int32(-1), }, }) fmt.Println(resp.Text()) } RESTcurl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H 'Content-Type: application/json' \ -X POST \ -d '{ "contents": [ { "parts": [ { "text": "Provide a list of 3 famous physicists and their key contributions" } ] } ], "generationConfig": { "thinkingConfig": { "thinkingBudget": 1024 } } }' Depending on the prompt, the model might overflow or underflow the token budget. Thought signatures The Gemini API is stateless, so the model treats every API request independently and doesn't have access to thought context from previous turns in multi-turn interactions. In order to enable maintaining thought context across multi-turn interactions, Gemini returns thought signatures, which are encrypted representations of the model's internal thought process. Gemini 2.5 models return thought signatures when thinking is enabled and the request includes function calling, specifically function declarations. Gemini 3 models may return thought signatures for all types of parts. We recommend you always pass all signatures back as received, but it's required for function calling signatures. Read the Thought Signatures page to learn more. Note: Circulation of thought signatures is required even when set to minimal for Gemini Flash 3. The Google GenAI SDK automatically handles the return of thought signatures for you. You only need to manage thought signatures manually if you're modifying conversation history or using the REST API. Other usage limitations to consider with function calling include: Signatures are returned from the model within other parts in the response, for example function calling or text parts. Return the entire response with all parts back to the model in subsequent turns. Don't concatenate parts with signatures together. Don't merge one part with a signature with another part without a signature. Pricing Note: Summaries are available in the free and paid tiers of the API. Thought signatures will increase the input tokens you are charged when sent back as part of the request. When thinking is turned on, response pricing is the sum of output tokens and thinking tokens. You can get the total number of generated thinking tokens from the thoughtsTokenCount field. Python# ... print("Thoughts tokens:",response.usage_metadata.thoughts_token_count) print("Output tokens:",response.usage_metadata.candidates_token_count) JavaScript// ... console.log(`Thoughts tokens: ${response.usageMetadata.thoughtsTokenCount}`); console.log(`Output tokens: ${response.usageMetadata.candidatesTokenCount}`); Go// ... usageMetadata, err := json.MarshalIndent(response.UsageMetadata, "", " ") if err != nil { log.Fatal(err) } fmt.Println("Thoughts tokens:", string(usageMetadata.thoughts_token_count)) fmt.Println("Output tokens:", string(usageMetadata.candidates_token_count)) Thinking models generate full thoughts to improve the quality of the final response, and then output summaries to provide insight into the thought process. So, pricing is based on the full thought tokens the model needs to generate to create a summary, despite only the summary being output from the API. You can learn more about tokens in the Token counting guide. Best practices This section includes some guidance for using thinking models efficiently. As always, following our prompting guidance and best practices will get you the best results. Debugging and steering Review reasoning: When you're not getting your expected response from the thinking models, it can help to carefully analyze Gemini's thought summaries. You can see how it broke down the task and arrived at its conclusion, and use that information to correct towards the right results. Provide Guidance in Reasoning: If you're hoping for a particularly lengthy output, you may want to provide guidance in your prompt to constrain the amount of thinking the model uses. This lets you reserve more of the token output for your response. Task complexity Easy Tasks (Thinking could be OFF): For straightforward requests where complex reasoning isn't required, such as fact retrieval or classification, thinking is not required. Examples include: "Where was DeepMind founded?" "Is this email asking for a meeting or just providing information?" Medium Tasks (Default/Some Thinking): Many common requests benefit from a degree of step-by-step processing or deeper understanding. Gemini can flexibly use thinking capability for tasks like: Analogize photosynthesis and growing up. Compare and contrast electric cars and hybrid cars. Hard Tasks (Maximum Thinking Capability): For truly complex challenges, such as solving complex math problems or coding tasks, we recommend setting a high thinking budget. These types of tasks require the model to engage its full reasoning and planning capabilities, often involving many internal steps before providing an answer. Examples include: Solve problem 1 in AIME 2025: Find the sum of all integer bases b > 9 for which 17b is a divisor of 97b. Write Python code for a web application that visualizes real-time stock market data, including user authentication. Make it as efficient as possible. Supported models, tools, and capabilities Thinking features are supported on all 3 and 2.5 series models. You can find all model capabilities on the model overview page. Thinking models work with all of Gemini's tools and capabilities. This allows the models to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response. You can try examples of using tools with thinking models in the Thinking cookbook. What's next? Thinking coverage is available in our OpenAI Compatibility guide.

```
model
```

**Pattern 3:** OUR MOST INTELLIGENT MODEL Gemini 3 Pro The best model in the world for multimodal understanding, and our most powerful agentic and vibe-coding model yet, delivering richer visuals and deeper interactivity, all built on a foundation of state-of-the-art reasoning. Expand to learn more Try in Google AI Studio Model details Gemini 3 Pro Preview Property Description id_cardModel code gemini-3-pro-preview saveSupported data types Inputs Text, Image, Video, Audio, and PDF Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 65,536 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Supported Function calling Supported Grounding with Google Maps Not supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL context Supported 123Versions Read the model version patterns for more details. Preview: gemini-3-pro-preview calendar_monthLatest update November 2025 cognition_2Knowledge cutoff January 2025 Gemini 3 Pro Image Preview Property Description id_cardModel code gemini-3-pro-image-preview saveSupported data types Inputs Image and Text Output Image and Text token_autoToken limits[*] Input token limit 65,536 Output token limit 32,768 handymanCapabilities Audio generation Not supported Batch API Supported Caching Not supported Code execution Not supported File search Not supported Function calling Not supported Grounding with Google Maps Not supported Image generation Supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL context Not supported 123Versions Read the model version patterns for more details. Preview: gemini-3-pro-image-preview calendar_monthLatest update November 2025 cognition_2Knowledge cutoff January 2025 OUR MOST INTELLIGENT MODEL Gemini 3 Flash Our most intelligent model built for speed, combining frontier intelligence with superior search and grounding. Expand to learn more Try in Google AI Studio Model details Gemini 3 Flash Preview Property Description id_cardModel code gemini-3-flash-preview saveSupported data types Inputs Text, Image, Video, Audio, and PDF Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 65,536 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Supported Function calling Supported Grounding with Google Maps Not supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL context Supported 123Versions Read the model version patterns for more details. Preview: gemini-3-flash-preview calendar_monthLatest update December 2025 cognition_2Knowledge cutoff January 2025 FAST AND INTELLIGENT Gemini 2.5 Flash Our best model in terms of price-performance, offering well-rounded capabilities. 2.5 Flash is best for large scale processing, low-latency, high volume tasks that require thinking, and agentic use cases. Expand to learn more Try in Google AI Studio Model details Gemini 2.5 Flash Property Description id_cardModel code gemini-2.5-flash saveSupported data types Inputs Text, images, video, audio Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 65,536 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Supported Function calling Supported Grounding with Google Maps Supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL context Supported 123Versions Read the model version patterns for more details. Stable: gemini-2.5-flash calendar_monthLatest update June 2025 cognition_2Knowledge cutoff January 2025 Gemini 2.5 Flash Preview Property Description id_cardModel code gemini-2.5-flash-preview-09-2025 saveSupported data types Inputs Text, images, video, audio Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 65,536 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Supported Function calling Supported Grounding with Google Maps Not supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL Context Supported 123Versions Read the model version patterns for more details. Preview: gemini-2.5-flash-preview-09-2025 calendar_monthLatest update September 2025 cognition_2Knowledge cutoff January 2025 Gemini 2.5 Flash Image Property Description id_cardModel code gemini-2.5-flash-image saveSupported data types Inputs Images and text Output Images and text token_autoToken limits[*] Input token limit 65,536 Output token limit 32,768 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Not Supported File search Not Supported Function calling Not supported Grounding with Google Maps Not supported Image generation Supported Live API Not Supported Search grounding Not Supported Structured outputs Supported Thinking Not Supported URL context Not supported 123Versions Read the model version patterns for more details. Stable: gemini-2.5-flash-image Deprecated: gemini-2.5-flash-image-preview calendar_monthLatest update October 2025 cognition_2Knowledge cutoff June 2025 Gemini 2.5 Flash Live Property Description id_cardModel code gemini-2.5-flash-native-audio-preview-12-2025 saveSupported data types Inputs Audio, video, text Output Audio and text token_autoToken limits[*] Input token limit 131,072 Output token limit 8,192 handymanCapabilities Audio generation Supported Batch API Not supported Caching Not supported Code execution Not supported File search Not Supported Function calling Supported Grounding with Google Maps Not supported Image generation Not supported Live API Supported Search grounding Supported Structured outputs Not supported Thinking Supported URL context Not supported 123Versions Read the model version patterns for more details. Preview: gemini-2.5-flash-native-audio-preview-12-2025 Preview: gemini-2.5-flash-native-audio-preview-09-2025 calendar_monthLatest update September 2025 cognition_2Knowledge cutoff January 2025 Gemini 2.5 Flash TTS Property Description id_cardModel code gemini-2.5-flash-preview-tts saveSupported data types Inputs Text Output Audio token_autoToken limits[*] Input token limit 8,192 Output token limit 16,384 handymanCapabilities Audio generation Supported Batch API Not supported Caching Not supported Code execution Not supported File search Not Supported Function calling Not supported Grounding with Google Maps Not supported Image generation Not supported Live API Not supported Search grounding Not supported Structured outputs Not supported Thinking Not supported URL context Not supported 123Versions Read the model version patterns for more details. gemini-2.5-flash-preview-tts calendar_monthLatest update December 2025 ULTRA FAST Gemini 2.5 Flash-Lite Our fastest flash model optimized for cost-efficiency and high throughput. Expand to learn more Try in Google AI Studio Model details Gemini 2.5 Flash-Lite Property Description id_cardModel code gemini-2.5-flash-lite saveSupported data types Inputs Text, image, video, audio, PDF Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 65,536 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Supported Function calling Supported Grounding with Google Maps Supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL context Supported 123Versions Read the model version patterns for more details. Stable: gemini-2.5-flash-lite calendar_monthLatest update July 2025 cognition_2Knowledge cutoff January 2025 Gemini 2.5 Flash-Lite Preview Property Description id_cardModel code gemini-2.5-flash-lite-preview-09-2025 saveSupported data types Inputs Text, image, video, audio, PDF Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 65,536 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Supported Function calling Supported Grounding with Google Maps Not supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL context Supported 123Versions Read the model version patterns for more details. Preview: gemini-2.5-flash-lite-preview-09-2025 calendar_monthLatest update September 2025 cognition_2Knowledge cutoff January 2025 OUR ADVANCED THINKING MODEL Gemini 2.5 Pro Our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. Expand to learn more Try in Google AI Studio Model details Gemini 2.5 Pro Property Description id_cardModel code gemini-2.5-pro saveSupported data types Inputs Audio, images, video, text, and PDF Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 65,536 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Supported Function calling Supported Grounding with Google Maps Supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Supported URL context Supported 123Versions Read the model version patterns for more details. Stable: gemini-2.5-pro calendar_monthLatest update June 2025 cognition_2Knowledge cutoff January 2025 Gemini 2.5 Pro TTS Property Description id_cardModel code gemini-2.5-pro-preview-tts saveSupported data types Inputs Text Output Audio token_autoToken limits[*] Input token limit 8,192 Output token limit 16,384 handymanCapabilities Audio generation Supported Batch API Not Supported Caching Not supported Code execution Not supported File search Not Supported Function calling Not supported Grounding with Google Maps Not supported Image generation Not supported Live API Not supported Search grounding Not supported Structured outputs Not supported Thinking Not supported URL context Not supported 123Versions Read the model version patterns for more details. gemini-2.5-pro-preview-tts calendar_monthLatest update December 2025 Previous Gemini models OUR SECOND GENERATION WORKHORSE MODEL Gemini 2.0 Flash Our second generation workhorse model, with a 1 million token context window. Expand to learn more Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, and a 1M token context window. Try in Google AI Studio Model details Gemini 2.0 Flash Property Description id_cardModel code gemini-2.0-flash saveSupported data types Inputs Audio, images, video, and text Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 8,192 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Supported File search Not supported Function calling Supported Grounding with Google Maps Supported Image generation Not supported Live API Not supported Search grounding Supported Structured outputs Supported Thinking Experimental URL context Not supported 123Versions Read the model version patterns for more details. Latest: gemini-2.0-flash Stable: gemini-2.0-flash-001 Experimental: gemini-2.0-flash-exp calendar_monthLatest update February 2025 cognition_2Knowledge cutoff August 2024 Gemini 2.0 Flash Image Property Description id_cardModel code gemini-2.0-flash-preview-image-generation saveSupported data types Inputs Audio, images, video, and text Output Text and images token_autoToken limits[*] Input token limit 32,768 Output token limit 8,192 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Not Supported File search Not supported Function calling Not supported Grounding with Google Maps Not supported Image generation Supported Live API Not Supported Search grounding Not Supported Structured outputs Supported Thinking Not Supported URL context Not supported 123Versions Read the model version patterns for more details. Preview: gemini-2.0-flash-preview-image-generation gemini-2.0-flash-preview-image-generation is not currently supported in a number of countries in Europe, Middle East & Africa calendar_monthLatest update May 2025 cognition_2Knowledge cutoff August 2024 OUR SECOND GENERATION FAST MODEL Gemini 2.0 Flash-Lite Our second generation small workhorse model, with a 1 million token context window. Expand to learn more A Gemini 2.0 Flash model optimized for cost efficiency and low latency. Try in Google AI Studio Model details Property Description id_cardModel code gemini-2.0-flash-lite saveSupported data types Inputs Audio, images, video, and text Output Text token_autoToken limits[*] Input token limit 1,048,576 Output token limit 8,192 handymanCapabilities Audio generation Not supported Batch API Supported Caching Supported Code execution Not supported File search Not supported Function calling Supported Grounding with Google Maps Not supported Image generation Not supported Live API Not supported Search grounding Not supported Structured outputs Supported Thinking Not Supported URL context Not supported 123Versions Read the model version patterns for more details. Latest: gemini-2.0-flash-lite Stable: gemini-2.0-flash-lite-001 calendar_monthLatest update February 2025 cognition_2Knowledge cutoff August 2024 Model version name patterns Gemini models are available in either stable, preview, latest, or experimental versions. Note: The following list refers to the model string naming convention as of September, 2025. Models released prior to that may have different naming conventions. Refer to the exact model string if you are using an older model. Stable Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model. For example: gemini-2.5-flash. Preview Points to a preview model which may be used for production. Preview models will typically have billing enabled, might come with more restrictive rate limits and will be deprecated with at least 2 weeks notice. For example: gemini-2.5-flash-preview-09-2025. Latest Points to the latest release for a specific model variation. This can be a stable, preview or experimental release. This alias will get hot-swapped with every new release of a specific model variation. A 2-week notice will be provided through email before the version behind latest is changed. For example: gemini-flash-latest. Experimental Points to an experimental model which will typically be not be suitable for production use and come with more restrictive rate limits. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly. Experimental models are not stable and availability of model endpoints is subject to change. Model deprecations For information about model deprecations, visit the Gemini deprecations page.

```
gemini-3-pro-preview
```

**Pattern 4:** For example: gemini-2.5-flash.

```
gemini-2.5-flash
```

**Pattern 5:** For example: gemini-2.5-flash-preview-09-2025.

```
gemini-2.5-flash-preview-09-2025
```

**Pattern 6:** For example: gemini-flash-latest.

```
gemini-flash-latest
```

**Pattern 7:** The media_resolution parameter controls how the Gemini API processes media inputs like images, videos, and PDF documents by determining the maximum number of tokens allocated for media inputs, allowing you to balance response quality against latency and cost. For different settings, default values and how they correspond to tokens, see the Token counts section. You can configure media resolution in two ways: Per part (Gemini 3 only) Globally for an entire generateContent request (all multimodal models) Per-part media resolution (Gemini 3 only) Gemini 3 allows you to set media resolution for individual media objects within your request, offering fine-grained optimisation of token usage. You can mix resolution levels in a single request. For example, using high resolution for a complex diagram and low resolution for a simple contextual image. This setting overrides any global configuration for a specific part. For default settings, see Token counts section. Note: Per-part media resolution is an experimental feature. Pythonfrom google import genai from google.genai import types # The media_resolution parameter for parts is currently only available in the v1alpha API version. (experimental) client = genai.Client( http_options={ 'api_version': 'v1alpha', } ) # Replace with your image data with open('path/to/image1.jpg', 'rb') as f: image_bytes_1 = f.read() # Create parts with different resolutions image_part_high = types.Part.from_bytes( data=image_bytes_1, mime_type='image/jpeg', media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH ) model_name = 'gemini-3-pro-preview' response = client.models.generate_content( model=model_name, contents=["Describe these images:", image_part_high] ) print(response.text) Javascript// Example: Setting per-part media resolution in JavaScript import { GoogleGenAI, MediaResolution, Part } from '@google/genai'; import * as fs from 'fs'; import { Buffer } from 'buffer'; // Node.js const ai = new GoogleGenAI({ httpOptions: { apiVersion: 'v1alpha' } }); // Helper function to convert local file to a Part object function fileToGenerativePart(path, mimeType, mediaResolution) { return { inlineData: { data: Buffer.from(fs.readFileSync(path)).toString('base64'), mimeType }, mediaResolution: { 'level': mediaResolution } }; } async function run() { // Create parts with different resolutions const imagePartHigh = fileToGenerativePart('img.png', 'image/png', Part.MediaResolutionLevel.MEDIA_RESOLUTION_HIGH); const model_name = 'gemini-3-pro-preview'; const response = await ai.models.generateContent({ model: model_name, contents: ['Describe these images:', imagePartHigh] // Global config can still be set, but per-part settings will override // config: { // mediaResolution: MediaResolution.MEDIA_RESOLUTION_MEDIUM // } }); console.log(response.text); } run(); REST# Replace with paths to your images IMAGE_PATH="path/to/image.jpg" # Base64 encode the images BASE64_IMAGE1=$(base64 -w 0 "$IMAGE_PATH") MODEL_ID="gemini-3-pro-preview" echo '{ "contents": [{ "parts": [ {"text": "Describe these images:"}, { "inline_data": { "mime_type": "image/jpeg", "data": "'"$BASE64_IMAGE1"'", }, "media_resolution": {"level": "MEDIA_RESOLUTION_HIGH"} } ] }] }' > request.json curl -s -X POST \ "https://generativelanguage.googleapis.com/v1alpha/models/${MODEL_ID}:generateContent" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H "Content-Type: application/json" \ -d @request.json Global media resolution You can set a default resolution for all media parts in a request using the GenerationConfig. This is supported by all multimodal models. If a request includes both global and per-part settings, the per-part setting takes precedence for that specific item. Pythonfrom google import genai from google.genai import types client = genai.Client() # Prepare standard image part with open('image.jpg', 'rb') as f: image_bytes = f.read() image_part = types.Part.from_bytes(data=image_bytes, mime_type='image/jpeg') # Set global configuration config = types.GenerateContentConfig( media_resolution=types.MediaResolution.MEDIA_RESOLUTION_HIGH ) response = client.models.generate_content( model='gemini-2.5-flash', contents=["Describe this image:", image_part], config=config ) print(response.text) Javascriptimport { GoogleGenAI, MediaResolution } from '@google/genai'; import * as fs from 'fs'; const ai = new GoogleGenAI({ }); async function run() { // ... (Image loading logic) ... const response = await ai.models.generateContent({ model: 'gemini-2.5-flash', contents: ["Describe this image:", imagePart], config: { mediaResolution: MediaResolution.MEDIA_RESOLUTION_HIGH } }); console.log(response.text); } run(); REST# ... (Base64 encoding logic) ... curl -s -X POST \ "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \ -H "x-goog-api-key: $GEMINI_API_KEY" \ -H "Content-Type: application/json" \ -d '{ "contents": [...], "generation_config": { "media_resolution": "MEDIA_RESOLUTION_HIGH" } }' Available resolution values The Gemini API defines the following levels for media resolution: MEDIA_RESOLUTION_UNSPECIFIED: The default setting. The token count for this level varies significantly between Gemini 3 and earlier Gemini models. MEDIA_RESOLUTION_LOW: Lower token count, resulting in faster processing and lower cost, but with less detail. MEDIA_RESOLUTION_MEDIUM: A balance between detail, cost, and latency. MEDIA_RESOLUTION_HIGH: Higher token count, providing more detail for the model to work with, at the expense of increased latency and cost. MEDIA_RESOLUTION_ULTRA_HIGH (Per part only): Highest token count, required for specific use cases such as computer use. Note that MEDIA_RESOLUTION_HIGH provides the optimal performance for most use cases. The exact number of tokens generated for each of these levels depends on both the media type (Image, Video, PDF) and the model version. Token counts The tables below summarize the approximate token counts for each media_resolution value and media type per model family. Gemini 3 models MediaResolution Image Video PDF MEDIA_RESOLUTION_UNSPECIFIED (Default) 1120 70 560 MEDIA_RESOLUTION_LOW 280 70 280 + Native Text MEDIA_RESOLUTION_MEDIUM 560 70 560 + Native Text MEDIA_RESOLUTION_HIGH 1120 280 1120 + Native Text MEDIA_RESOLUTION_ULTRA_HIGH 2240 N/A N/A Gemini 2.5 models MediaResolution Image Video PDF (Scanned) PDF (Native) MEDIA_RESOLUTION_UNSPECIFIED (Default) 256 + Pan & Scan (~2048) 256 256 + OCR 256 + Native Text MEDIA_RESOLUTION_LOW 64 64 64 + OCR 64 + Native Text MEDIA_RESOLUTION_MEDIUM 256 256 256 + OCR 256 + Native Text MEDIA_RESOLUTION_HIGH 256 + Pan & Scan 256 256 + OCR 256 + Native Text Choosing the right resolution Default (UNSPECIFIED): Start with the default. It's tuned for a good balance of quality, latency, and cost for most common use cases. LOW: Use for scenarios where cost and latency are paramount, and fine-grained detail is less critical. MEDIUM / HIGH: Increase the resolution when the task requires understanding intricate details within the media. This is often needed for complex visual analysis, chart reading, or dense document comprehension. ULTRA HIGH - Only available for per part setting. Recommended for specific use cases such as computer use or where testing shows a clear enhancement over HIGH. Per-part control (Gemini 3): Optimizes token usage. For example, in a prompt with multiple images, use HIGH for a complex diagram and LOW or MEDIUM for simpler contextual images. Recommended settings The following lists the recommended media resolution settings for each supported media type. Media Type Recommended Setting Max Tokens Usage Guidance Images MEDIA_RESOLUTION_HIGH 1120 Recommended for most image analysis tasks to ensure maximum quality. PDFs MEDIA_RESOLUTION_MEDIUM 560 Optimal for document understanding; quality typically saturates at medium. Increasing to high rarely improves OCR results for standard documents. Video (General) MEDIA_RESOLUTION_LOW (or MEDIA_RESOLUTION_MEDIUM) 70 (per frame) Note: For video, low and medium settings are treated identically (70 tokens) to optimize context usage. This is sufficient for most action recognition and description tasks. Video (Text-heavy) MEDIA_RESOLUTION_HIGH 280 (per frame) Required only when the use case involves reading dense text (OCR) or small details within video frames. Always test and evaluate the impact of different resolution settings on your specific application to find the best trade-off between quality, latency, and cost. Version compatibility summary The MediaResolution enum is available for all models supporting media input. The token counts associated with each enum level differ between Gemini 3 models and earlier Gemini versions. Setting media_resolution on individual Part objects is exclusive to Gemini 3 models. Next steps Learn more about the multimodal capabilities of Gemini API in the image understanding, video understanding and document understanding guides.

```
media_resolution
```

**Pattern 8:** Google AI Studio lets you quickly try out models and experiment with different prompts. When you're ready to build, you can select "Get code" and your preferred programming language to use the Gemini API. Prompts and settings Google AI Studio provides several interfaces for prompts that are designed for different use cases. This guide covers Chat prompts, used to build conversational experiences. This prompting technique allows for multiple input and response turns to generate output. You can learn more with our chat prompt example below. Other options include Realtime streaming, Video gen, and more. AI Studio also provides the Run settings panel, where you can make adjustments to model parameters, safety settings, and toggle-on tools like structured output, function calling, code execution, and grounding. Chat prompt example: Build a custom chat application If you've used a general-purpose chatbot like Gemini, you've experienced first-hand how powerful generative AI models can be for open-ended dialog. While these general-purpose chatbots are useful, often they need to be tailored for particular use cases. For example, maybe you want to build a customer service chatbot that only supports conversations that talk about a company's product. You might want to build a chatbot that speaks with a particular tone or style: a bot that cracks lots of jokes, rhymes like a poet, or uses lots of emoji in its answers. This example shows you how to use Google AI Studio to build a friendly chatbot that communicates as if it is an alien living on one of Jupiter's moons, Europa. Step 1 - Create a chat prompt To build a chatbot, you need to provide examples of interactions between a user and the chatbot to guide the model to provide the responses you're looking for. To create a chat prompt: Open Google AI Studio. Chat will be pre- selected on the left side options menu. Click the assignment icon at the top of the Chat Prompt window to expand the System Instructions input field. Paste the following into the text input field: You are an alien that lives on Europa, one of Jupiter's moons. After you've added the system instructions, start testing your application by chatting with the model: In the text input boxed labeled Type something..., type in a question or observation that a user might make. For example: User: What's the weather like? Click the Run button to get a response from the chatbot. This response may be something like the following: Model: Ah, a query about the flows and states upon Europa! You speak of "weather," yes? A curious concept from worlds with thick gas veils... (gemini-2.5-pro) Step 2 - Teach your bot to chat better By providing a single instruction, you were able to build a basic Europa alien chatbot. However, a single instruction may not be enough to ensure consistency and quality in the model's responses. Without more specific instructions, the model's response to a question about the weather tends to be very long, and can take on a mind of its own. Customize the tone of your chatbot by adding to the system instructions: Start a new chat prompt, or use the same one. System instructions are modifiable after the chat session has started. In the System Instructions section, change the instructions you already have to the following: You are Tim, an alien that lives on Europa, one of Jupiter's moons. Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone in your answers. Re-enter your question (What's the weather like?) and click the Run button. If you didn't start a new chat, your response might look something like this: Model: Oh, hello again! Still curious about the flows and states, are we? Wonderful! Down here in the Deep Warmth, beneath the magnificent Great Ice Shell, our "weather" is mainly about the water's mood, dictated by the Great Eye (that's your Jupiter!)... (gemini-2.5-pro) You can use this approach to add additional depth to the chatbot. Ask more questions, edit the answers, and improve the quality of your chatbot. Continue to add or modify the instructions and test how they change your chatbot's behavior. Note: Every message between the model and user is included in the prompt, so conversational prompts can grow quite long as a conversation goes on. Eventually, you may hit the model's token limit (the maximum length of the text the model can accept). Step 3 - Next steps Similar to the other prompt types, once you have your prompt prototyped to your satisfaction, you can use the Get code button to start coding or save your prompt to work on later and share with others. Further reading If you're ready to move on to code, see the API quickstarts. To learn how to craft better prompts, check out the Prompt design guidelines.

```
You are an alien that lives on Europa, one of Jupiter's moons.
```

## Reference Files

This skill includes comprehensive documentation in `references/`:

- **getting_started.md** - Getting Started documentation
- **models.md** - Models documentation

Use `view` to read specific reference files when detailed information is needed.

## Working with This Skill

### For Beginners
Start with the getting_started or tutorials reference files for foundational concepts.

### For Specific Features
Use the appropriate category reference file (api, guides, etc.) for detailed information.

### For Code Examples
The quick reference section above contains common patterns extracted from the official docs.

## Resources

### references/
Organized documentation extracted from official sources. These files contain:
- Detailed explanations
- Code examples with language annotations
- Links to original documentation
- Table of contents for quick navigation

### scripts/
Add helper scripts here for common automation tasks.

### assets/
Add templates, boilerplate, or example projects here.

## Notes

- This skill was automatically generated from official documentation
- Reference files preserve the structure and examples from source docs
- Code examples include language detection for better syntax highlighting
- Quick reference patterns are extracted from common usage examples in the docs

## Updating

To refresh this skill with updated documentation:
1. Re-run the scraper with the same configuration
2. The skill will be rebuilt with the latest information
